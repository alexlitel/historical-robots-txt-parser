[![PyPI version shields.io](https://img.shields.io/pypi/v/historical-robots-txt-parser.svg)](https://pypi.python.org/pypi/historical-robots-txt-parser/) [![PyPI license](https://img.shields.io/pypi/l/historical-robots-txt-parser.svg)](https://pypi.python.org/pypi/historical-robots-txt-parser/)


# Historical Robots.txt Parser

This is a small Python package that parses the historical robots.txt files from the Internet Archive's Wayback Machine and coerces the data into a CSV file for tracking addition and removal of `Allow` and `Disallow` rules by timestamp, path and user-agent. It's a fairly narrow use case but may be helpful to researchers or journalists.

## Requirements
* Python 3.7 or later

## How to use

### 1. Install module
1. Install module using

### 2. Clone repo
